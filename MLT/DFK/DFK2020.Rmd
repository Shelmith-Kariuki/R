---
title: <center><strong>Introduction to Machine Learning in the Tidyverse</strong></center>
author: <center><strong><h1>[Shelmith Nyagathiri Kariuki](https://github.com/Shelmith-Kariuki)</h1></strong></center>
date: <center><strong><h2>February 25, 2019</h2></strong></center>
output:
  html_document:
    toc: yes
    toc_depth: '6'
    code_folding: hide
---

This training was inspired by [Alison Hill](https://twitter.com/apreshill) and [Garrett Grolemund](https://twitter.com/StatGarrett)'s workshop training, given at [rstudio::conf2020](https://rstudio.com/conference/).

We will cover machine learning using the tidymodels package. tidymodels is a "meta-package" for modeling and statistical analysis that share the underlying design philosophy, grammar, and data structures of the tidyverse.

The packages that are part of the tidymodels packages, that we will be using today include:

+ rsample: has infrastructure for resampling data so that models can be assessed and empirically validated.

+ recipe: is a general data preprocessor with a modern interface. It can create model matrices that incorporate feature engineering, imputation, and other help tools.

+ parsnip: is a tidy, unified interface to creating models.

+ yardstick: contains tools for evaluating models.

The machine learning process is as indicated in the image below.
!insert image here



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE,fig.width = 12)

```

### 0. Install the libraries required
<br>
```{r, }

## create a vector of packages to be installed
pkgs <- c("tidyverse","data.table","DT","lubridate","ggthemes","kernlab")

## Check if there are packages you want to load, that are not already installed. 
miss_pkgs <- pkgs[!pkgs %in% installed.packages()[,1]] 

## Installing the missing packages
if(length(miss_pkgs)>0){
  install.packages(miss_pkgs)
}
## Loading all the packages
invisible(lapply(pkgs,library,character.only=TRUE))

## Remove the objects that are no longer required
rm(miss_pkgs)
rm(pkgs)

## Install and load the tidymodels package.
# require(devtools)
# devtools::install_github("tidymodels/tidymodels")
library(tidymodels)
```


### 1. Read in the dataset
<br>
We will be using the [Financial Inclusion in Africa](https://zindi.africa/competitions/financial-inclusion-in-africa) found on the [Zindi](https://zindi.africa/) platform. 

Zindi is an African data science platform that gives African data scientists a place to learn new skills, grow through their competitions and their community and access work opportunities.

Zindi also gives organisations and governments access to world-class machine learning and AI solutions through their talented community of data scientists.
```{r,}

df <- readr::read_csv("FinancialInclusion/Train_v2.csv")

```
Because of my laptop's computation power, we will only use Kenyan data
```{r,}

df <- df %>% 
  filter(country == "Kenya") %>% 
  select(-uniqueid, -year) ## dropping variables we do not need. The Kenyan dataset is all 2018, so we drop that variable

```

### 2. Check if we have missing values
We do not have missing values in any of the variables.
```{r,}

miss_vals <- df %>% 
  map(., sum(is.na(.)))

```

### 3. Split the dataset into train and test data
We will use the `initial_split()` function from the `rsample` package to split the data into train (75% of the original data) and test data (25% of the original data).

```{r,}
## Set seed
set.seed(1801)

## Specify the split object
split <- initial_split(df, prop = 0.75)
split
```

<4551/1517/6068> indicates that we have 4551 observations in the train data, 1517 in the test data, and 6068 overall.

```{r,}
## Extract the training dataset
train_df <- split %>% training()

## Extract the testing data
test_df <- split %>% testing()
```

### 4. Preprocess the train and test datasets.
We will `prep` the training data using the recipes package, then use the `bake` function to implement the recipe object on both the terain and test datasets. 

A recipe is built in three steps:

+ Start the recipe()
+ Define the variables involved.
+ Define pre-processing step by step, using the step_* functions

Some of the sep functions include:

+ step_bagimpute(): Imputation via Bagged Trees
+ step_knnimpute(): Imputation via K-Nearest Neighbors
+ step_bin2factor(): Create a Factors from A Dummy Variable
+ step_dummy(): Dummy Variables Creation
+ step_string2factor(): Convert Strings to Factors
+ step_center(): Centering numeric data
+ step_scale(): Scaling Numeric Data
+ step_normalize(): Center and scale numeric data
+ step_mutate(): Adds new variables using mutate
+ step_zv(): Removes variables that do not have variance

More of these functions can be found on [this recipes  documentation](https://tidymodels.github.io/recipes/reference/index.html).
  
The step functions work with selectors. These are helper functions for selecting sets of variables. They include:

+ all_predictors(): represents the explanatory/independent variables
+ all_outcomes():represents the outcome/dependent variables. 
+ all_numeric(): represents the numeric variables.
+ all_nominal(): represents the categorical variables. 
+ add_role(): adds an additional role to variables that already have a role in the recipe. It does not overwrite old roles, as a single variable can have multiple roles.
+ update_role(): alters an existing role in the recipe or assigns an initial role to variables that do not yet have a declared role.

Create the recipes object
```{r,}
rec <- recipe(bank_account~., train_df)
rec
```

Create the pre-processing object
```{r,}
rec <- rec %>% 
  step_string2factor(all_nominal())%>% 
  step_integer(all_numeric())%>% 
  step_zv(all_predictors())
rec
```

Create an object that details the pre-processing steps carried out.
```{r,}
trained_rec <- rec %>% prep(train_df)
trained_rec
```

Implement the pre-processing object on both the train dataset and test dataset using the `bake` function.
```{r,}
train_df <- trained_rec %>% bake(train_df)
test_df <- trained_rec %>% bake(test_df)

```

### 5. Fit a model using the train dataset

The parsnip model is used to fit the model

This process involves two steps:

+ Specifying the model
  + Pick a model
  + Set the engine
  + Set the mode (if need be)
  
+ Fitting the model

More details about these steps can be found on [this parsnip documentation.](https://tidymodels.github.io/parsnip/articles/articles/Models.html)

#### 5.1 Logistic model

##### 5.1.1 Fit the model
```{r,}
logistic_model <- logistic_reg() %>% #1. Pick the model
                  set_engine("glm") %>% #2. Set the engine
                  set_mode("classification") %>%  #3. Set the mode if need be.
                  fit(bank_account~., train_df) #4. Fit the model.

## Obtain a tidy dataframe of the object
logistic_tidy_df <- logistic_model %>% tidy.model_fit()
logistic_tidy_df
```

##### 5.1.2 Generate predicted values
```{r,}

logistic_pred <- logistic_model %>% 
  predict(new_data = test_df) %>% 
  mutate(actual = test_df$bank_account) 
logistic_pred

```

##### 5.1.3 Confusion Matrix
```{r,}
confusion_matrix <- logistic_pred %>% 
  conf_mat(actual, .pred_class)
confusion_matrix
```

##### 5.1.4 Accuracy
```{r,}
accuracy_logistic <- logistic_pred %>% 
  accuracy(actual, .pred_class)
accuracy_logistic
```

#### 5.2. Random Forest

##### 5.2.1 Fit the model
```{r,}
start_time <- Sys.time()
rf_model <- rand_forest(mtry = 5000) %>% #1. Pick the model
                  set_engine("randomForest") %>% #2. Set the engine
                  set_mode("classification") %>%  #3. Set the mode if need be.
                  fit(bank_account~., train_df) #4. Fit the model.

end_time <- Sys.time()

print(end_time - start_time)


## Obtain a tidy dataframe of the object
# rf_tidy_df <- rf_model %>% tidy.model_fit()
# rf_tidy_df
```

##### 5.2.2 Generate predicted values
```{r,}

rf_pred <- rf_model %>% 
  predict(new_data = test_df) %>% 
  mutate(actual = test_df$bank_account) 
rf_pred

```

##### 5.2.3 Confusion Matrix
```{r,}
confusion_matrix <- rf_pred %>% 
  conf_mat(actual, .pred_class)
confusion_matrix
```


##### 5.2.4 Accuracy
```{r,}
accuracy_rf <- rf_pred %>% 
  accuracy(actual, .pred_class)
accuracy_rf
```


#### 5.3 SVM
##### 5.3.1 Fit the model
```{r,}
start_time <- Sys.time()
svm_model <- svm_rbf() %>% #1. Pick the model
                  set_engine("kernlab") %>% #2. Set the eng ine
                  set_mode("classification") %>%  #3. Set the mode if need be.
                  fit(bank_account~., train_df) #4. Fit the model.

end_time <- Sys.time()

print(end_time - start_time)


## Obtain a tidy dataframe of the object
# svm_tidy_df <- svm_model %>% tidy.model_fit()
# svm_tidy_df
```

##### 5.3.2 Generate predicted values
```{r,}

svm_pred <- svm_model %>% 
  predict(new_data = test_df) %>% 
  mutate(actual = test_df$bank_account) 
svm_pred

```

##### 5.3.3 Confusion Matrix
```{r,}
confusion_matrix <- svm_pred %>% 
  conf_mat(actual, .pred_class)
confusion_matrix
```


##### 5.3.4 Accuracy
```{r,}
accuracy_svm <- svm_pred %>% 
  accuracy(actual, .pred_class)
accuracy_svm
```

